---
title: "RPI Analysis"
Author: "Danielle Ethier"
date: "March 2020"
output: html_document
---

Code to dowload the raw data for plotting prior to analysis. 

```{r load packages, echo = FALSE, message = FALSE}

require(INLA)		# for analyzing data
require(reshape) 	# for summarizing/reshaping data
require(dplyr)# for summarizing data (better than reshape for most things)
require(doBy)

# function to call in various analysis functions that draw on the BMDE database
source("I:/R-functions/functions.r")

# Source other required R Scripts
source("./Scripts/rpi.a.ImportData.R")


```

```{r set common parameters among sites, echo = FALSE, message = FALSE}

# Set max year to analyze *** CHANGE EVERY UPDATE
max.yr <- 2019  

# output directory for all analysis output files
out.dir <- paste("I:/r-functions/work/rpi/TrendAnalysis/output/", max.yr, "/", sep = "")

# create output directory if it does not exist
dir.create(out.dir, showWarnings=FALSE, recursive=TRUE)

# Create directory for sql files, which are used by Denis to create web output
dir.create(paste(out.dir, "sql_scripts", sep = ""), showWarnings=FALSE, recursive=TRUE)

# get list of species common names, to be used later
sp.names <- subset(bscdata.getSpeciesTable(datasource="bmdedata", authority = "rpi", showall=FALSE, sortorder=1), select = c("species_code","sort_order", "english_name", "scientific_name", "french_name"))


```

```{r set site specific parameters, echo = FALSE, message = FALSE}

# Read in Analysis Parameters File to get list of sites to analyze; contains site code, site name, seasons and sometimes years to analyze
anal.param <- read.csv("./data/RPI_Analysis_Parameters.csv")

```

```{r analyze, echo = FALSE}

# LOOP THROUGH ROWS IN anal.param, OR JUST SPECIFY EACH AND RUN MANUALLY
# Each row is a separate site AND season (i.e., sites that count both seasons are in two rows)

for(i in 1:nrow(anal.param)){  # loop through sites and seasons 
  
   #i<-46 #site is not downloading properly. i
   #i<-66 #re-download Lucky Peak
   #i<-79 #re-download Commissary Ridge
    i<-48

   site <- as.character(anal.param[i, "SiteCode"])
data.type <- as.character(anal.param[i, "data.type"])

print(paste("analyzing site: ", site, sep = ""))

sites = NA #c("HawkCount-389","HawkCount-625") # vector, if there are more than 1 site to combine.

seas <- as.character(anal.param[i,"seas"])
min.yr.filt <- anal.param[i,"min.yr.filt"]
max.yr.filt <- anal.param[i,"max.yr.filt"]


# DOWNLOAD DATA FROM SERVER - SITE AND SEASON SPECIFIC
# Read data from server and write to file: OUTPUTS RAW DATA FILES TO DIRECTORY SPECIFIED.  Could put this elsewhere - loop through and save data for all sites to specified folder; then other manipulation/checks could be run as well.
# This also filters bad dates using the bscdata.filterBadDates(in.data, project=1013) function

read.data(site = site, sites = sites, out.dir = out.dir, data.type = data.type)


} # end of site/season loop

```

