---
title: "RPI Analysis"
author: "Danielle Ethier"
date: "March 2020"
output: html_document
---

Code to extract the RPI data after the data maniputaion function is applied. This will allow us to inspect sites as they will used for the anlaysis. 

```{r load packages, echo = FALSE, message = FALSE}

require(INLA)		# for analyzing data
require(reshape) 	# for summarizing/reshaping data
require(dplyr)# for summarizing data (better than reshape for most things)
require(doBy)

# function to call in various analysis functions that draw on the BMDE database
source("I:/R-functions/functions.r")

# Source other required R Scripts
source("./Scripts/rpi.a.ImportData.R")
source("./Scripts/rpi.b.DataManip.R") # also filters station coverage for daily data. This results in daily total which are used for the anlysis. Houly totals are no longer used for the analysis. 
source("./Scripts/rpi.c.DailyWindows.R")
source("./Scripts/rpi.d.SeasonalWindows.R")
source("./Scripts/rpi.e.FilterNyears.R")
source("./Scripts/rpi.f.MigrationWindowSummary.R")

```

```{r set common parameters among sites, echo = FALSE, message = FALSE}

# Set max year to analyze *** CHANGE EVERY UPDATE
max.yr <- 2019  

# output directory for all analysis output files
out.dir <- paste("I:/r-functions/work/rpi/TrendAnalysis/output/", max.yr, "/", sep = "")

# create output directory if it does not exist
dir.create(out.dir, showWarnings=FALSE, recursive=TRUE)


# get list of species common names, to be used later
sp.names <- subset(bscdata.getSpeciesTable(datasource="bmdedata", authority = "rpi", showall=FALSE, sortorder=1), select = c("species_code","sort_order", "english_name", "scientific_name", "french_name"))

#there are some parameters here that are a hold  over from older analysis. 
write.manip.data = FALSE # write manipulated data to file? THis does ge written to a file but not until later in the code. 
filter.day = TRUE		# filter by daily migration windows?
day.pctile1 = 2.5		# if filter.day = TRUE, lower percentile
day.pctile2 = 97.5		# if filter.day = TRUE, upper percentile
write.daily.windows = TRUE	# if filter.day = TRUE, write daily windows (start/end hour) to file
filter.seas = TRUE		# Filter by seasonal windows?
filter.years = TRUE 		# filter data by min and max years
seas.pctile1 = 2.5		# if filter.seas = TRUE, lower percentile
seas.pctile2 = 97.5		# if filter.seas = TRUE, upper percentile
write.windows = TRUE		# if filter.seas = TRUE, write seasonal windows to file?
center.var = TRUE		# center date variables prior to analysis?
doy.k = 4			# for GAM model, start number nodes for day
time.k = 4			# for GAM model, start number nodes for year
results.code = "RPI"
weight.var = "DurationInHours"
yr.incr = 10  			# increment for automatically generated time periods
write.indices = TRUE		# write estimated annual indices to file?
write.trends = TRUE		# write estimated trends to file?
write.mig.wind = TRUE		# write data to calculate migration windows to file?
write.plot = TRUE		# write migration window plots to file?
station.pctile1 = 2.5
station.pctile2 = 97.5
write.indices.sql = TRUE
write.trends.sql = TRUE
write.mig.wind.sql = TRUE
```

```{r set site specific parameters, echo = FALSE, message = FALSE}

# Read in Analysis Parameters File to get list of sites to analyze; contains site code, site name, seasons and sometimes years to analyze
anal.param <- read.csv("./data/RPI_Analysis_Parameters.csv")

```

```{r analyze, echo = FALSE}

# LOOP THROUGH ROWS IN anal.param, OR JUST SPECIFY EACH AND RUN MANUALLY
# Each row is a separate site AND season (i.e., sites that count both seasons are in two rows)

#for(i in 1:nrow(anal.param)){  # loop through sites and seasons 
for(i in 76:89){  # loop through sites and seasons 
  
#set the value of i for testing  
#i<-1 
  
site <- as.character(anal.param[i, "SiteCode"])
data.type <- as.character(anal.param[i, "data.type"])

print(paste("analyzing site: ", site, sep = ""))

sites = NA #c("HawkCount-389","HawkCount-625") # vector, if there are more than 1 site to combine.

seas <- as.character(anal.param[i,"seas"])
min.yr.filt <- anal.param[i,"min.yr.filt"]
max.yr.filt <- anal.param[i,"max.yr.filt"]

#Used data downloaded previously 
#read.data(site = site, sites = sites, out.dir = out.dir, data.type = data.type)

# READ IN DATA AND MANIPULATE - SITE AND SEASON SPECIFIC

in.data <- NULL
in.data <- read.csv(paste(out.dir, site, ".", seas, ".raw.csv", sep = ""))
in.data <- filter(in.data, !is.na(datetime))
in.data$datetime <- as.POSIXct(in.data$datetime)
in.data$year <- as.POSIXlt(in.data$datetime)$year + 1900
in.data$doy <- as.POSIXlt(in.data$datetime)$yday

# DATA MANAGEMENT - SITE AND SEASON SPECIFIC
# drop years where necessary (eg., last year in dataset might not be completely entered or perhaps based on what years were analyzed previously. Assign max year as what is listed in anal.param, or maximum year in database

# if no reason to drop any years at beginning (in anal.param), use min year in dataframe
  min.yr.filt <- ifelse(is.na(min.yr.filt), min(in.data$year), min.yr.filt)
  max.yr.filt <- ifelse(is.na(max.yr.filt), max.yr, max.yr.filt)

if(filter.years) {
	in.data <- filter(in.data, year >= min.yr.filt & year <= max.yr.filt)
  } # end of if filter years

print("year range:"); print(range(in.data$year))

if(max(in.data$year) == max.yr) { #continue only if the max year in database is what it should be

# DATA.MANIP function
# includes filtering station, daily and seasonal windows (auto-generated).  Also amalgamated hourly data into daily totals.  Note that for daily data, daily migration windows are not filtered.  Assumes consistent hours sampled each day

manip.dat <- data.manip.hourly(in.data = in.data, site = site, out.dir = out.dir,	write.manip.data = write.manip.data, seas = seas, station.pctile1 = station.pctile1, station.pctile2 = station.pctile2, day.pctile1 = day.pctile1, day.pctile2 = day.pctile2, write.daily.windows = write.daily.windows, seas.pctile1 = seas.pctile1, seas.pctile2 = seas.pctile2, write.windows = write.windows, data.type = data.type)

min.yrs.detect = (max.yr.filt-min.yr.filt)/2 # number of years a species must be detected on to be analyzed


# check how many species were in dataset before year filter:

print(paste("Number species before yr filter: ", length(unique(manip.dat$SpeciesCode)), sep = ""))
  
print(as.character(unique(manip.dat$SpeciesCode)))

print(paste("number years in dataset: ", (length(unique(manip.dat$year))+ 1),
	sep = ""))

print(paste("suggested min.yrs: ", round((length(unique(manip.dat$year)) + 1)/2)),
	sep = "")

# DROP SPECIES THAT AREN'T DETECTED DURING SPECIFIED NUMBER OF YEARS

manip.dat <- droplevels(filter.nyrs(in.data = manip.dat, min.yrs.detect = min.yrs.detect))

# check how many species after filter

print(paste("Number species after yr filter: ", length(unique(manip.dat$SpeciesCode)), sep = ""))
  
print(as.character(unique(manip.dat$SpeciesCode)))

# DROP SPECIES THAT AREN'T DETECTED with mean 10 individuals per year.

tmp <- group_by(manip.dat, SpeciesCode, year) %>%
	summarise( 
	mean.count = mean(ObservationCount/DurationInHours), 
	max.count = max(ObservationCount),
	tot.count = sum(ObservationCount)) %>%
 	summarise( 
	mean.mncount = mean(mean.count), 
	max.mxcount = mean(max.count),
	mean.totcount = mean(tot.count))

sp.list <- subset(tmp$SpeciesCode, tmp$mean.totcount >= 10)
  
manip.dat <- droplevels(filter(manip.dat, SpeciesCode %in% sp.list))

write.csv(manip.dat, file = paste(out.dir, site, ".", seas, ".FilteredData.csv", sep = ""), 
	row.names = FALSE)

  } #close if max year
} #close i loop

```
